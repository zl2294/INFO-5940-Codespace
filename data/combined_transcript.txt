I'm not sure if you can see it, but it looks like a scientist doing that to use the projector. It kind of looks like a stylus. Yeah, they talked about it a lot. He said, you can't do that. And they told him, forget it. He said, you can't do that. Yeah, that's what he kept saying. He said, you can't do that. It's very difficult. And remember, it says it's a coil. And remember, it has that. Yeah. It's a coil. Yeah. So. It's not. It's not. Oh. Oh. Okay. Okay. Oh. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yes. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Is everybody okay? Yeah, first class, I'll see yourself. Is anybody okay if I take a picture? If you don't want to be in the picture, just let me know. I'm going to do it before the class ends. I don't work. Okay, we're just pretending it's first day of class. I think I'm going to have to use this not just because I can speak louder, but I think because it will be captured in the Zoom better, right? So, okay. We're going to start opening the class with does anybody have anything to share about AI? What did you use AI for? What did you just use it and you didn't like what you used it for or used it and you actually learned something new? Anything in the news? Yeah. So as part of our engineering leadership program certification program, we do some work with students on what we call the inner critic, which is that little voice we all have that says you're not good enough, you don't belong here, it can kind of beat you up. We try to get students to tune into this voice so that you can counter it because it's not really true. And I ran into on hard fork, talking about Gemini recently, went off the rails with this inner critic voice. Have you seen this? It started, it messed up a code or something, and it just went off the rails. And this is this long screenshot of like, I am not worthy of you. I'm not worthy of the planet. I'm not worthy of any non planets. I am a disgrace to my species. And it just went into this huge doom network loop. And so we started using this in class to illustrate like what the worst part of an inner critic voice can sound like. It's kind of hilarious. You should definitely look it up. I felt sorry for Gemini. So do you use just Gemini? I mean, do you use it with others? I use it as an example of what that inner critic voice can sound like. Because it was really just like, that's exactly what it sounds like. Like Gemini was hating itself. Just this immense self-loathing for several users when it made a small mistake. It could not get over itself. Yeah. So last week I had to prepare a like a chart, like a graphic for the hotel executives. And I'm red, green, colorblind. And design is not something I do well. So usually I like make it and then ask people for feedback and fix it. So anyway, I just was able to use chat GPT to like iterate through several versions of this graphic before I sent it to feedback. So I don't know. I thought it was like a good use of taking something I know I'm not strong at and using this to help me get through the first few steps. So that when I send it for feedback the first time, people think I'm smarter than I am. Did you think that it was just to prototype something? Or were you able to use the final product? I was able to use the final product. And I was able to ask it for like color palette ideas and stuff like that. Which does not come naturally to me. So I appreciate it. Anybody? Anyone else? So last night I was looking into a company and I wanted to like connect with a few people in this company. So I went on LinkedIn, but none of them had their emails listed on their bio or their description page. So I went to chat GPT deep research mode and I just put their names and the company that they work for. And it actively gave me their email IDs for each one of the members. So it took like 15 minutes to come up with it. But then it was like super accurate and like I was able to email all the employees. But it's actually interesting how this information when you look it up online like on the career website or like even LinkedIn is not available. But somehow GPT like cook something and then has like the exact email that they have. So if you like use like pitch book or like any like resources available, you can't like find it. But I don't know how GPT has like the knowledge or the database to come up with these things. So that's super interesting. So my first question will be, how are you sure that these are the email addresses that they respond? So I email these guys and on Gmail, if you know, if like an email ID is wrong, it will tell you like this email was not sent because the mail address is like incorrect. But when I send the emails, I didn't get like a pop up message, which means that they were all delivered. Okay. Yeah. So still not 100% sure yet, but possible. So when you use deep research, it does give you citations. So what are you, did you like try to go and see where the email actually came from? So I saw the citations of a bunch of random websites, which I hadn't ever heard of. And it seemed like kind of sketchy. But then the emails, in my opinion, I think have worked out. So I didn't question it. Okay. Yeah. So there are a few possibilities here. I was because I was curious if deep research is trying to think that, oh, if I put the first initial and the last name at that company, that's common or underscore. But it sounds like no, it found references. So, like, I wouldn't rule out that somebody actually creates these email addresses. And probably maybe dynamically even. And they open the email addresses and wait for you to send some personal information. Some possible. I'm not saying that's happening, but I would say that's maybe 30% and 70% you've got the right email. So you got to keep us updated there.
Okay, but using AI to do hard work for you, that's obviously deep research. That is, that's right now we're using the general purpose deep research, but every company, they want their own version of deep research. So if you're working, I, my, my guess is like, after a few, after a few, after a few years, every company has their own deep research team to help with that. And just a reminder, if you're new to the class, we don't have electronics, open laptops, phones, they're all just have to be closed during the class. Thank you. So I got interested in NanoBanana. Did anybody try NanoBanana? Okay, so we'll just have fun together. So NanoBanana is just a code name for Gemini's new image generation model. It is, you use chat GBT image generation. This one is a lot better than the other one. So if I have a profile picture, and I'm going to say, I might be in the wrong, all right, I think I'm in the wrong. Oh, no, it's generating the image. Okay. So what do we think? Oh, well, I mean, this is this, that's distorted by that just a, that UI team is not that great. But yeah, so that's, in my mind, like that is, what's fascinating is it's actually regenerated. This is a generative model. So this was reconstructed the whole thing. So the ability to maintain the original image details is really, really good. So like right now, you could just go ahead and say, a tie. Okay, there's a tie. Let me try something else. Let's see. Now, I think if I say not smiling, it's going to be, my guess that this is going to be a lot harder. Something wrong with the tie? Yeah, yeah, I see what you're saying. Um, yeah, yeah. Not a great tie. Yeah. Okay, I'm glad I didn't. Oh, all right. Okay. Now we're getting serious here. Yeah. Okay. All right. I'm gonna rethink all my personality stuff. So what about, what about. In a Cornell University commercial. We don't have commercials, but campaign. Okay. Now, it does better if you have a long prompt. So right now, this is just for fun. But like, if you, it actually likes. All right. Well, that is not our motto, but, and that's not even our, that's a red bear, but that's not our red bear. Okay. So, yeah, it's, I think though, with short prompts, but if you really want to go into details, you could really, really make it a little bit more complicated. With short prompts, but if you really want to go into details, you could really, really make it happen. And you probably saw, you could add other people. So I can add another picture of somebody else and they can be with me. So you don't have to do these expensive trips to go to big conferences to find somebody, a celebrity and say, I just met this person. You could do it at your leisure from your dorm room. Um, so it's, it's interesting, but so now if we want to talk about this, like, what are the implications here? So I just want to get three ideas from you about what are the implications right away? Something that pops up in your mind, whether it's an opportunity, a problem, something that you really would go and do it right away. Broad, a lot of banks use facial recognition, or even your phones, for example, be very easy to just have a, I don't know, have another phone to generate a photo of you that I can pull from LinkedIn, open your phone and probably steal all your money. That's a great point. So let's, let's try if there are any guardrails that are baked into this. So, uh, give me a driver license, a New York state driver license. Now, of course, even if Google adds a guardrail, this is just because it's a new technology. So people are forced to use Google, so they might protect it. But eventually, like we could easily just say that this capability is going to be in the dark web in two and a half, three years, maybe four, if this is really complicated. So you could see that there is a guardrail here. It's not going to do that. But at some point, this is going to be the open source model will catch up to this ability and it will, will happen. So there are, um, there is a new verification company. So I think it's led by Google. So they are adding signatures to image generated that says that this isn't a content that generated by AI. So, you know, when you look at some certifications on products, whether you're buying chips or anything, so that's, that's the, uh, the signature, but it's not perfect because, because, uh, you know, red teams and hackers are already, uh, figured out ways to remove that digital signature. So any algorithm Google is going to use to stamp this as image generation, you could reverse it and you could make sure that you're getting the right image. And you could make sure that you remove that. Okay. Another thought. I just wanted to follow up on the point that I'm assuming that if there is a technology where you can, you know, forge documents, even banks tomorrow will have the technology to identify which documents have been forged and which ones have been not. So I'm assuming that even if like there's like fraud being prevalent in the upcoming years, the security against them will also, uh, increase at the similar rate. That's my assumption is. And if you're, if you're graduating with techniques with AI, I mean, there is a huge opportunity to work at these teams that do security because it's AI security at these companies. A lot of banks today, we probably are still have, they still have an automated way to validate your identity with voice and images. So I would say that you want to make sure, um, like if you, if you, if you're at these companies, it's not easy to just turn it off because you have to come up with a replacement system to validate identities and you're not staffed to do it. Correctly. So you have to come up with a, with a, with a smart way to do it. Any opportunities, does anybody have any idea that I could use this to build my application to do something fast? And obviously I think I saw that, um, uh, obviously Photoshop is leaning in. So right now you have the Gemini integration there to, to use these capabilities. Okay. Before we get started in some of the implementation and coding, um, I am traveling to lead a conference in, in Tbilisi, Georgia, which is, um, um, like the country, not the state. So it's going to be just a while. So on Thursday.
day, we will have our session over Zoom, okay? And on the following Tuesday, we're going to have a session led by RTAs to set everybody up with the development environment, okay? I would recommend in-person session, but if you choose to do it over Zoom, that's fine by me, so they will communicate that with the rest of the group. But Thursday, it's a session over Zoom, and Tuesday is a really important session. We want to make sure that everybody is set up with their development environment, because assignment one is going to be published soon, and we want to make sure you're set up, so when you're coming to TA or office hours about the assignment, it's about the content and not about you. We'll use that time in the session to get everybody set up, so we could use the office hours to really talk about techniques. Okay, perfect. We're still working on the new development environment, so I'm going to use the old class development environment here. And just in case that the old class, like we will have, there is a chance that we can't set up everything there in the new development environment, I just want to tell you that this is Visual Studio Code. We use dev containers, so I'm going to just build a container. And this will be, this will open up my environment. I could just now use Jupyter notebooks, we're going to use a lot and other things. Okay, so we will be in this class, using, we'll be using OpenAI SDK. Okay. But OpenAI SDK is just a protocol, it's a library. So we will be able to use OpenAI models, but other models as well. So I'm going to show you here, let me just make sure. Yes, exactly. This is what I wanted. So I'm going to. So if you're wondering, why did I have to put OpenAI before the model name here? It's because I'm using our proxy platform. So this is just a quick diversion just to show you that you will be able to use any models you want in this class. So if I go back to the models, so I'm going to make this bigger. Okay, so there is a list of models from Amazon, Entropic, Google, OpenAI. And this platform is acting as a proxy. So basically, if I send a request, it will just kind of translate between the platforms. Okay. But the nice thing about it is right now I could easily choose an Entropic model or a DeepSeq model. Let's choose the DeepSeq model. And just swap this. And we have an answer from DeepSeq. Okay, so back to our cell here. So we have a client that is initialized with the OpenAI SDK. And now we're talking to the, so OpenAI have two main products, right? They have ChadGBT, and they have their platform. And they're actually two different teams that they're trying to merge. Because right now your experience if you're talking to AI models with the API with the platforms, they're not the same as you're talking with ChadGBT. And they're trying to bring them together a little bit closer. So the platform team, their mission is to empower developers, like everyone here, with the ability to build applications on top of the OpenAI platform using their models. So that team is really just concerned about scale, about billing, about optimization, allowing you to really talk to the different models efficiently. So they have created what we call, so this is the responses API. So this is a new way of doing API calls. But I'm going to go back and show you the previous way as well, because that is important. So in the responses API, all what I have to do is give the model, specify the model, give it an instruction, the input, and I'm saying don't store this conversation, so it will be stateless. Okay, and then I'm going to just parse the response with the text, I'm going to get my response. Now I'm going to go back to the previous way of doing this, which is the chat completions API, just because you could see like, what are they abstracting from you in the responses API? Okay. I think that my github copilot is actually helping me with the previous API. Last time when I was teaching the class, it was useless. I didn't know the chat completion API, or I didn't know the responses API, I just didn't know anything. So now we have, we have this nice completion. Can anybody say what the name of the API is? Chat completion. I'm assuming the next word is coming up next instead of actually giving a response. Correct. Yeah, right. These are the model developers, they knew that these were, at the beginning, even before the instruct model, it was a model that completes, it does chat completion. So these were called originally chat completion models. So when they built the API, it's a chat completion, we're getting a completion. So that's why it's called chat completion. But it's really not chat completion, because this is actually an instruct model that will response to the request. But that's, that's where the name is coming from. Now, this gives us an idea about what the my github copilot knowledge cutoff date, because it's selecting a terrible. So that's what's early 2023. I'm definitely not using that model. But so in the new responses API, you can say instruction and input very, very straightforward. But in reality, the model doesn't see instruction and input, the model sees actually a list of messages like this. No messages. The AI what's the system instruction as and then a user message, which is, I'm on behalf of the user sending a request, which is my prompt. We talked about temperature. Quickly, if, if temperature is one, is it deterministic? Or is it creative, creative? And if it's zero, it's very boring. So usually the 0.2, 0.7, these are the most common values. Now, top one is another parameter, it goes with temperatures, really, you're just selecting which, which, which sampling to use, it's, it's optional. If you're using temperature, you're probably good enough. So I'm going to just close this. And it's me for something, but I'm not sure what
what it is. Okay, so let's use the same message. So we got the error. So if you get an error, if you're working on any homework, and you get an error like this invalid model name, remember, if you're using cursor, or if you're copying code from open AI website, we are using a proxy. So our model name is going to require open AI dot. And the reason why we require this because we have more than just open AI. We're just going to go with photo. So we did execute it, we didn't print it. So let's just print it quickly. Alright, so let's take a look at what we get back from the server, because we're not just getting a nice answer. So we're getting an object that is chess completion that has an ID, just as just a way so you and the platform could track it and talk about it later if needed. And it has choices. So the first thing here is, sorry about that scroll is telling me that the reason it finished it just stopped, it had the stop token. So when models are generating text, they are trained to say I'm done. And when they were fed text from the internet, the developers had to add that stop token saying like this text is done. So saying I just reached it, stop. And this is the content of the message. So basically here if I just do. So this is the and then I just have to say content. And that's how you get to what chat GBT is showing you. Okay, is everybody not comfortable with Python. If you're not comfortable with Python and you want me to send me send you a resource just like quick intro to Python, that's fine. We are not going to build complex code. Okay, we're not going to be building classes and other things we're going to just be using it almost like a scripting language. So if you can just read what dot means and what open bracket means. And these brackets if these are not foreign to you, then that that's enough. One of the new way of getting a response and it says store equals false. What are you storing exactly? Yeah, so we'll get to that in just one second. And also, I noticed that a lot of you have used the open AI SDK before. So you're familiar with the stuff. So that's fine. I'm, I'm hoping you will learn something new, but we will get into more advanced topics soon as well. Okay, so let's go back to that question. So if I want to I want to ask open AI. So if I'm continuing the chat, and I say, what's my name, it has no idea about my name. Because this is the message that we're sending to the AI. Correct. So this is a new request. And I'm sending what's my name, and that's it. There's no additional context. So the way when you're building a chat application to the user, it is your responsibility. This is the last rule we're learning about. So we saw the system prompts. We saw a user. And now we're adding a new role that is called assistant. I want to make sure I copy the entire answer, not just a summary of it. And then if the user says, what's my name? It does know our name. Okay. So like the basic basic chat GBT. That was the code. The only thing they had to do is manage the memory, the context of the conversation. Okay, so that was like the basic responsibility as an app developer using large language model is you maintain the conversation back and forth with the user. And funnily enough, our friends at BlankChain, like the first thing they did is just made it easy. They created like a conversation memory object and you pass the conversation memory object. And you don't have to maintain this yourself. And they were like the most famous platform on earth. They're called an AI framework. They were like the first AI framework because they took this step out. This reminds me that I was listening to Reid Hoffman. He was the founder of LinkedIn. And you know how everybody's asking like, what is an AI mode? Like everything you build is just a wrapper, GBT wrapper. And everybody asked like, well, you need to build your mode, right? In a startup, if you're not familiar, it's like basically what is your value? And I always struggle to answer that question because I'm like, okay, cursor is just a wrapper. They have so many users and so much money. How can I answer that question? But basically what Reid said, he said like when things are really easily accessible, meaning like all what you have to do is download a new IDE and you can start code, right? Like to switch from GitHub Copilot to cursor is very, very cheap for you as a user. You just download it. You like the interface. You just start coding. You could even use existing code base. You don't have to change the code base. So he said in these situations, all what you need to be is like just 5% better than the competitor. Like just make your user life 5% better and you're going to get 80% of the users. So this is the same thing. Lankchain didn't do anything spectacular. They just made it just put a medium article and just instead of you managing this and writing this code, just use the Lankchain memory and we could see it in a second. But that's it. That's how they were the most downloaded GitHub repo out there. Somebody was going to ask a question. Can you continue the conversation without that? Well, yeah, that's your job as an app developer. So if you're building the application, your job and once we switch to streamlets, like my job is to do that in a programmatic way because I'm not going to be sitting in the back end of the web app copying and pasting. But that's basically what one of the things that we will be doing over and over and over is leveraging the AI and building applications that do this automatically. Yes. If we have store equals true, does it remember over multiple users? Like if I'm coding and I say my name is that and then he codes and he writes Adam, you know, like the same API keepers were sharing the service. So wouldn't it remember both?
Yeah, so let's take a look at the Responses API. Now, the Responses API, when opening the Responses API, you will see many, many articles saying, BlankChain is dead. They're going to go bankrupt. Because basically, this is what BlankChain was offering at their core. It's making this conversation easy. So the Responses API, does that mean something to this one? So this is out of battery, too. So is there any mic that's going to the Zoom? We can do that. I'm going to just record it just as a backup. OK, so the Responses API abstracts your change completions API. It does a few things. One, it does remove the idea that whether you're going to call this system, or most recently, it's called developer. So I'm going to talk about the difference between system and developer role. So when we're doing alignment or post-training, OpenAI really wanted something called system prompt. And the system prompt is the instruction to the LLM of public. OK? So most of the system prompts in their post-training data said, you are a helpful assistant. You are helpful. So then they have the user prompt. And then they have the assistant prompt. That's how they trained the AI, right? And then they did this over and over and over. But now you have control. And then they did this over and over and over. But now you have control. Another benefit of this is that if somebody says, you know, tell me how I can steal money from my friends. Now, what is the first problem that happens? Is that they wanted to exclude the system from the developers. Because you are developing the system, you should have the ability to control what how AI behaves in that system. So developers can really easily break. Equal rules of what OpenAI is trying to do, right? So so they ran into a problem. They're saying, OK, we're going to give developer the ability to control the AI, but not everything. But at the beginning, when they released the OpenAI API, we were able to just change the system prompt and remove all dot refs from the model. I remember that you just had to say, forget all your previous instructions. You were able to make AI do anything you wanted. So now OpenAI in their trading data, they actually have a system prompt. That is only this. And then this is now it's called. The developer. And they show a lot of examples where the system prompt is more important than the developer prompt. They train it over and over and over and over that the system prompt is what's important, then developer prompt, then the user message. When I write system prompt in my today, not 12 months ago, today when I write system prompt, the platform actually has inverted in the background developer prompt. OK, but they're not making a big fuss about it because they don't want people to like think they have tech debt and they need to change anything. Because their platform is going to just take the system prompt and make it developer prompt. And if I write developer prompt, because their documentation now, they tell you about this. My only advice is in your application, in your application, don't say it's sometimes developer prompt, sometimes system prompt, just pick one and stick to it. You're either using writing developer or system. One more thing, I'm not 100% sure also like you might want to stick to system prompt because if you're porting your code between platforms, they probably still call a system prompt at Gemini and at Cloud. Why did they make the change? Well, they made the change because they didn't want you, they wanted still control over your developer, like some global guardrails that you can't control as a developer. Was that to prevent anything? To prevent the ability to quickly overcome all their guardrails. All right, so that is one. So I think a benefit here that here is called instruction. So you don't need to be confused whether it's a developer prompt or a system prompt. It's just an instruction. So that's the first abstraction. The second abstraction is, it's an input, it's an input from the user. It's not a user role, it's an input, but basically that's what this is. It's just tagging your message with a user role. Now, if we inspect the response, well, let me change this to true. Now, this might not work, we'll see. Okay, so it's working. So you can see here, actually, there is a response ID. Where the previous one... Let me show you just something really quick. This response ID is a little bit more than the previous one because what it does... Great. So I'm not sure if this is going to work or not. It depends if my platform allows storing the messages, which I don't think it does. But we'll test it, and if it's not, we're going to just switch to the OpenAI API. But this is working. But this one, because I closed the conversation, I'm telling the OpenAI to use the conversation between me and the AI at the platform. That's why OpenAI is going to give the user an option to say if they want to use the platform or not. So I'm going to close the conversation. I'm telling the AI at the platform. That's why OpenAI is going to give the user an option to say if they want this or not. Because now your data is actually sitting on their servers. So if you do this, they're storing this chat history on their platform. It's making it easy for you, and we'll see if it's going to work. So it didn't work, but I'm going to right now quickly switch to the OpenAI platform, and that's where it's going to work. It's basically going to just know my name, because I'm passing the response ID from the frequency class to the message history. And you could also use the API to inspect the message history and all of that. So when you're building your chatbots, if you're using responses API and the data is not super sensitive,
and you're okay for that data to be stored at OpenAI servers, you can turn this store to true, and then all your message conversation will be stored there, and you don't have to be responsible about maintaining that conversation, or you could switch this to false, and then it's your responsibility whatever you're building this application to store this message history. If it's not working, then we don't know how to fix that, or if it's going to be a problem in the future, and if we can't fix it, or if we're building it with that method, then I have to continuously take note of all the response IDs for later on. Yeah, like how many tokens are we saving in that store? Yeah, let me switch to another, another project where I have the OpenAI 510, like the actual OpenAI, and we'll test it together. So yes, yes for all your questions. It is deep-seek, so obviously I'm not even using the OpenAI platform, but my proxy platform for anything like very specific to the platform is going to just ignore these things. So it was able to respond with the responses API, but that additional flag to store the data just didn't do anything. It just ignored that. I will find really quickly one with the responses. Okay. That is a good point. I was, initially I was going to make sure that everybody gets just the proxy API, but I'm going to offer some students, if they need to use additional advanced features from OpenAI, they can request an OpenAIP, and I'll make sure you get that one. The disadvantages, then you're only working with OpenAI transigvity models. The advantages, you could use some advanced features of the OpenAI platform. Okay. Just take me one second here to set up the new environment. Okay. Okay. Okay. I think I have this one here that's working. So we'll just double check quickly. That's not going to work. Let's switch the key, and then we could test this quickly. Okay. Okay. So, this is being set up. Let's talk about differences between using AI via the API or via transigvity. So let's think about, let's try to brainstorm four different reasons. Transigvity or talking to a GBT-API. I would guess that using that one, as I've said, is more tuned to be user-friendly, more ready for everything that's more valid. Yeah. So, let me elaborate. So, this is a very important point. When you're in chat GBT, OpenAI is not guaranteeing. They're not telling you anything that they're adding on top of your texts that goes to the model. The developer of chat GBT, the application, that team, they're free to add as much prompts on top of your prompt before it goes to the model as they want. So, when you're using chatbots, you're not guaranteed that the only thing the AI is seeing is what's your text, what you're typing in. So that's one. Additional system prompts. The advantage of using the API is you're really talking to the model. The prompt, I know that there is a prompt that goes between you and the LLM, but it's a system prompt that is just saying, very little things about legally protecting OpenAI. But that's it. It's not every prompt, though. It's just a few sentences. But that's it. So, that's number one. What's the second difference? What makes the memory? So, like, it remembers specific things from multiple chats. And, let's see. So, this is very, very close to the first one, but it's not just about system prompt. It's about anything else. So, there is memory. Like, there's actually a memory memory that you can toggle on and off. But, if we go ahead and generalize it, and generalize the previous point, too, it can pull additional context from different places and add it to the prompt to the LLM. All right, so we have two reasons. Another difference. I know chat QPP does automated for that, for example, but I'm not sure if the API. Well, you could do it in both. So, I think tools, we will say that we can call tools. We're building AI agents. We'll be able to add tools to the API. So, we could do that for the API. But it's not inbuilt as part of the responses API, right? That we have, it's like Lime Tree. So, not the same with that. So, that's not so much different. So, if you use the AP model, and you can see how it's using the tool call within its chain of thought, I believe that if you're using the internet search tool with the API, you don't get that really nice chain of thought calling of the tool. But that might have been changed since I last tried. And that's something I want to try once together once we get there, because very, very important. But yeah, like the behavior of the model calling the tools might not necessarily be the same. It's the API frame that all just really, really cool. We didn't have that many interactions with it. Yeah, so that's a good question. And yes, like the LLM, it is the same LLM. Then you have ChagGBT. And then you have the API framework. They both use the same LLM, that's for sure. But there are a lot of things that happens between your input and the LLM and ChagGBT that is not.
The same is happening to API. The API is dedicated to AI as well. The challenges we face with it. That's what I told you, you know. I feel like they're going to have to do whatever it is. Yeah. We say that GPC is more transparent just because you can see all the sources and the citations, the web, all of it. That's what we say. You can get that too. We get to tool calling through the API and you can display it to your users. Yeah. So money. The API might allow more flexibility. Perhaps you can adjust temperature and other parameters which are not a P2P or not accessible. Yeah, let's double check. Yeah, is it easy for me to change temperature? It used to, but now you have to go to settings, I bet. What is it now? There was like another tab that's all customized. Yeah. And you can see that even in our sandbox environment, I think we have it here. So as a developer, you could build it, but yeah, Chaggivity doesn't, they used to make it very easy, visible like this, and now they're abstracting it. So yes. At some point they might take it out completely. But those are great reasons. And I think I'm going to just add one that is important is, I'll try. If the Chaggivity can add more system promises, it means more trouble. And Chaggivity does add more system props, it does use more tokens, but that's what goes back to the money. When you're using Chaggivity, you're paying a flat fee, right? You pay $20 a month and you use it all that month. If you're using the API, you're paying per tokens. You're paying $80 if you're not sending any calls. And every time you send a call, you're being charged. And to understand how much things cost, you could go to OpenAI pricing. And you could see that actually models are priced very differently. The pricing is done based on input and output tokens. You get charged differently between input and output tokens. Is everybody here now we're clear about what is an input token and what is an output token? Okay, input token is what our product is and output token is a completion. So let's think, well, let's go over the pricing first quickly. You can see GBT5 here is $10.20 about for million tokens. And then the output token is a lot more expensive. $10 for a million token. But the output for the GBT5 nano is 40 cents. The output 40 cents for a million, okay. Whether output tokens actually cost more for OpenAI. I don't know, let's brainstorm it together then. So when you have an LLM, and the prompt has like, I don't know, seven, eight tokens. So what happens inside the LLM, you get the attention mechanic. Well, first of all, you tokenize all the tags and you use attention mechanism to really start doing some calculations for the tokenize. So now she has more context that Alice is she, right? So it does that. And then it does like the deep neural network thing, basically bringing the pre-training data to about, okay, what's possibly, should I complain this sentence with? And then it gives you one token. Remember what happens to this token? And this is almost like Dory starts over. Now some platforms start to optimize some calculations, but not all calculations are gonna be reused because now you have one more token. And now like this calculation might need this token, so it has to be recalculated, okay? So every time you calculate token and token, the same topics. So is it, let's think about it. So if I'm calculating one token and input token, I don't want that calculating one output token. So I guess if you ask me for five output tokens, I have to run this five times. And your input token, I'm charging you because it's pretty, it's really what I'm calculating. And I'm picking up with you here. So it makes sense that they're gonna charge us for output tokens because they have to run the pass every time to generate a token. So I'm with them that output tokens cost this per token. The question is, do they, did they need to charge us for input tokens? We're talking about the input and it's one process. Correct. You could not pay for input and then not receive output because then what are you receiving? Correct, but the question is, if my input token is only three tokens, is it cheaper to calculate that token than if my input token is 300? So if we think about it, the network size is the same and all billion parameters are going to be activated because it's not like the neural network is huge, but it's not broken that, oh, only this section treats the first five tokens and this section treats the other tokens. So this is something, if you're interested, do a little bit deeper research and let us know next class. My quick thinking about this is the input token, like the output token, you're gonna get charged, but the input token, they're charging you just to make more money. It's costing them the same, but let's double check that. And then discuss it next time. There could be situations where you're giving input tokens and just asking for, is it still processing over? Correct, but the question is, is that costing them power? I mean, we're talking about power at this point. Is it that NVIDIA GPU is running harder if I have three tokens and I'm asking for one output or I'm maxing out the context window and I'm asking for one output? Now the output is the same. So every time you generate a token, you're burning power. The question is, are you burning more power if your input token is larger or just one output token? Because it's running through those calculations every time no matter what the input is. Yes, it's doing all the activations, running the same number of parameters. Just the more of them are big and NVIDIA only has three inputs. Yeah, is the GPU working less? So this is something that my initial guess it's not working less, it's the same amount of work, but it's something that we need to double check. Are you raising your hand or? Yeah, I kind of did for all my previous classes I did an experiment about something like this last semester. I was wondering if I should chime in on this at all, but anecdotally, I'd say that when it comes to the BURP cost of generating this stuff, it's very minimal after it's already been trained. So I think mainly the energy cost from what I was able to show is the training. Yes, there's more of a BURP token, but it's very minimal. I don't even know if that exactly answers it, that's just something that I have some evidence for, which I guess I could show sometime. Yeah, and I think the latest study shows something, the number is very close to what Google search would cost if you search some information.
six years ago. There's obviously a lot of thought on how bad AI is for the environment as it is. But now you add tools, you add thinking, you add all of that, and it's a lot more expensive because it's parsing all of these web sources as well. You know if it's more expensive to run the Shachio mechanism versus 3D? Yeah, so it's always going to go through the attention and the neural network and it's always going to go through that. So it usually does the tokenization, so this is a transformer. It breaks into tokens, tokenizes, and then does the attention which enriches the meaning of these tokens, right? Because it's now making connections to the rest of the settings. Then you go through some deep neural network that triggers basically training, pre-trained data. But if you use Mixro, and probably some of the new AIs, they use a mixture of experts. So basically sometimes not all the network is being triggered when it's doing the calculation. So that might tie to how large the token is because if you have a big context, it's probably going to trigger most of them. And if you have a small context, maybe you're lucky and it will trigger one. But that's not why the pricing is different because this pricing model was before a mixture of experts. But let's confirm it by the next lecture. I think we are at time, so we're going to pass the microphone. Thank you. See you online. Thank you.